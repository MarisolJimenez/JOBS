{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys, os\n",
    "import glob\n",
    "import time\n",
    "import fnmatch\n",
    "import os\n",
    "import xlwings as xw\n",
    "from datetime import date\n",
    "start_time = time.time()\n",
    "print('1/14.- Reading the Current Backlog Combined Data')\n",
    "df1 = pd.read_excel(\"dataATS.xlsx\",dtype={'PART': str})\n",
    "#df1 = pd.read_excel(\"dataAIT.xlsx\", header=2, dtype={'PART': str})\n",
    "df1['SHIP_LOC'] = df1['SHIP_LOC'].str[:3] #delete possible dc with an -\n",
    "df1['SHIP_TO'] = df1['SOLD_TO'] #Copy AE column to AF\n",
    "print('2.- Filtering file for ATS')\n",
    "df1 = df1[(df1['EVM_AIT2'] == 'ATS') | (df1['EVM_AIT2'] == 'AIT')| (df1['EVM_AIT2'] == 'LS') | (df1['EVM_AIT2'] == 'PARTS')] \n",
    "#subset print(df1.shape)\n",
    "#df1 = df1[(df1['EVM_AIT2'] != '')] #subset print(df1.shape)\n",
    "#df1 = df1[(df1['EVM_AIT2'] != 'Circular Economy')]#subset print(df1.shape)\n",
    "df1['SSD'] = pd.to_datetime(df1['SSD'])\n",
    "df1['MATL_TYPE']=np.where(df1['MATL_TYPE']== 'Included','Simple',df1['MATL_TYPE'])\n",
    "#df1.MATL_TYPE.unique()\n",
    "\n",
    "print('3.- Inserting columns')\n",
    "df1 = df1.drop(['QOH'], axis = 1)\n",
    "df1.rename(columns={'NETTABLE_QOH': 'QOH'},inplace=True)\n",
    "\n",
    "df1.insert(2, 'Trilogy Or Merlin', 'MERLIN')\n",
    "df1.insert(3, 'Material Avail Date > CRSD', '') #J column formula\n",
    "df1.insert(5, 'SKU Short(QTYs On Hand) DBL & CRSD <=+4', '')\n",
    "df1.insert(6, 'SO Short(QTYs On Hand) DBL & CRSD <=+4', '')\n",
    "df1.insert(7, 'Missing CRSD Wk', '')\n",
    "df1.insert(8, 'CRSD WK', '')\n",
    "df1.insert(9, 'TPC SKU/DC/WK Key', '')\n",
    "df1[\"SKU DC Key\"]=df1[\"PART\"]+df1[\"SHIP_LOC\"]\n",
    "df1.insert(13, 'Blended ASP $', '')\n",
    "df1.insert(14, 'EXT Part ASP $', '')\n",
    "df1.insert(19, 'Oracle Intransit(Trilogy IR/ISO)', '')\n",
    "df1.insert(20, 'Oracle Intransit(Trilogy=>Merlin or Merlin=>Trilogy)', '')\n",
    "df1.insert(21, 'ZCM ASN QTY Supply', '')\n",
    "df1.insert(22, 'ZCM Commits WK1', '')\n",
    "df1.insert(23, 'ZCM Commits WK2', '')\n",
    "df1.insert(24, 'ZCM Commits WK3', '')\n",
    "df1.insert(25, 'ZCM Commits WK4', '')\n",
    "df1.insert(26, 'ZCM Commits WK5', '')\n",
    "df1.insert(27, 'WK1 TPC Receipt QTY', '')\n",
    "df1.insert(28, 'Valid TPC Supply', '')\n",
    "df1.insert(29, 'TPC Commits WK1', '')\n",
    "df1.insert(30, 'TPC Commits WK2', '')\n",
    "df1.insert(31, 'TPC Commits WK3', '')\n",
    "df1.insert(32, 'TPC Commits WK4', '')\n",
    "#df1.insert(33, 'ZCM ASN/Commit Net WK1 supply', '')\n",
    "df1.insert(34, 'ZCM ASN QTY and WK1 Net Supply', '')\n",
    "df1.insert(35, 'ZCM Net Commits WK2', '')\n",
    "df1.insert(36, 'ZCM Net Commits WK3', '')\n",
    "df1.insert(37, 'Valid TPC Net Supply WK1', 0)\n",
    "df1.insert(38, 'TPC Net Commits WK2', 0)\n",
    "df1.insert(39, 'TPC Net Commits WK3', 0)\n",
    "df1.insert(40, 'Missing Commits TPC/ZCM WK2', '')\n",
    "df1.insert(41, 'Net Supply capture by CRSD SKU/DC/WK', '')\n",
    "df1.insert(42, 'Missing ASN Supply Due to CRSD WK', '')\n",
    "df1.insert(43, 'Missing Commit Supply Due to CRSD WK', '')\n",
    "df1.insert(44, 'Merlin/Trilogy Missing Supply Total', '')\n",
    "df1.insert(45, 'SKU/DC Total Supply(OH only)', '')\n",
    "df1.insert(46, 'QTY Short to Sales Orders(CUM) Only INV', '')\n",
    "df1.insert(47, 'QTY Short to Sales Orders(NON-CUM) Only INV', '')\n",
    "df1.insert(48, 'EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM) Only INV)', '')\n",
    "df1.insert(49, 'SKU/DC Total Supply(On Hand+ALL ASN)', '')\n",
    "df1.insert(50, 'QTY Short to Sales Orders(CUM) (On Hand+ALL ASN', '')\n",
    "df1.insert(51, 'QTY Short to Sales Orders(NON-CUM) On Hand+ALL ASN', '')\n",
    "df1.insert(52, 'EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM)) On Hand+ALL ASN', '')\n",
    "df1.insert(53, 'SKU/DC Total Supply ALL ASNs+Commits(No Missing Commits)', '')\n",
    "df1.insert(54, 'QTY Short to Sales Orders(CUM) including Missing Commits+Commits+ALL ASNs', '')\n",
    "df1.insert(55, 'QTY Short to Sales Orders(NON-CUM) w/ Commits+Missing Commits+ALL ASNs', '')\n",
    "df1.insert(56, 'EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM)) INV w/ Commits+Missing Commits+ALL ASNs', '')\n",
    "df1.insert(57, 'SKU/DC Total Supply(OH+ALL ASNs+all 3wk Commtsh)', '')\n",
    "df1.insert(58, 'QTY Short to Sales Orders(CUM) OH+ALL ASNs+all 3wk Commts', '')\n",
    "df1.insert(59, 'QTY Short to Sales Orders(NON-CUM) OH+ALL ASNs+all 3wk Commts', '')\n",
    "df1.insert(60, 'EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM)) OH+ALL ASNs+all 3wk Commts', '')\n",
    "#df1.insert(61, 'Config Printer SKU', '')\n",
    "#df1.insert(62, 'Config Printer SKU/DC', '')\n",
    "df1.insert(63, 'Config Printer SKU/DC/WK Key', '')\n",
    "#df1.insert(64, 'Org Part INV-Part Simple Config Total QOH', '')\n",
    "df1.insert(65, 'Config SKU/DC WO Supply Simple Config', 0)\n",
    "#df1.insert(66, 'Config SKU/DC INV-Simple Config', '')\n",
    "#df1.insert(67, 'QOH (Part Simple Config + BASE+ WO Supply)', '')\n",
    "\n",
    "#create week COLUMN Z\n",
    "today = date.today()\n",
    "coming_sunday = today + datetime.timedelta(days=-today.weekday()-1, weeks=1) #this sunday sunday 1\n",
    "coming_sunday = coming_sunday.strftime('%Y-%m-%d') #format it\n",
    "coming_monday = today + datetime.timedelta(days=-today.weekday(), weeks=1)#this monday monday 1\n",
    "coming_monday = coming_monday.strftime('%Y-%m-%d') #format it\n",
    "next_sunday = today + datetime.timedelta(days=-today.weekday()-1, weeks=2)#sunday 2 \n",
    "next_sunday = next_sunday.strftime('%Y-%m-%d') #format it\n",
    "next_monday = today + datetime.timedelta(days=-today.weekday(), weeks=2)#monday2\n",
    "next_monday = next_monday.strftime('%Y-%m-%d') #format it\n",
    "following_sunday = today + datetime.timedelta(days=-today.weekday()-1, weeks=3) #sunday 3\n",
    "following_sunday = following_sunday.strftime('%Y-%m-%d') #format it\n",
    "\n",
    "mask1 = (df1['CRSD'] <= coming_sunday) #filter for wk1\n",
    "mask2 = (df1['CRSD'] >= coming_monday) & (df1['CRSD'] <= next_sunday) #filter for wk2\n",
    "mask3 = (df1['CRSD'] >= next_monday) & (df1['CRSD'] <= following_sunday) #filter for wk3\n",
    "\n",
    "#define which week the CRSD is in Week 1, 2, or 3\n",
    "df1['CRSD WK'] = np.where(mask1, 'WK1',\n",
    "                 np.where(mask2, 'WK2',\n",
    "                 np.where(mask3, 'WK3','')))\n",
    "\n",
    "#JUST LEAVE VALUES WHEN MAT TPYE IS SIMPLE\n",
    "df1['CRSD WK']=np.where(df1['MATL_TYPE']== 'Simple',df1['CRSD WK'],'')\n",
    "\n",
    "#COLUMN AA, BLUE, COLUMN USUALLY DONE/COPIED AT THE SAME TIME AS RED COLUMN\n",
    "df1['TPC SKU/DC/WK Key'] = df1[\"SKU DC Key\"] + df1[\"CRSD WK\"]\n",
    "\n",
    "#lookup Delivery Creation Date\n",
    "print('4.- Reading the In Play Report')\n",
    "df2 = pd.read_csv('In Play Report.csv')\n",
    "df2 = df2[['SO_LINE','Delivery Creation Date']]\n",
    "df2=df2.drop_duplicates(subset='SO_LINE',keep='first') #df.drop_duplicates as some SO LINES have 2 dates\n",
    "#first was kept since thats how it was done in excel\n",
    "df1 = pd.merge(df1, df2, on='SO_LINE', how='left')\n",
    "df1.rename(columns={'Delivery Creation Date':'SO Line Dropped to DC for Delivery'}, inplace=True)\n",
    "df1['SO Line Dropped to DC for Delivery']=pd.to_datetime(df1['SO Line Dropped to DC for Delivery'])\n",
    "\n",
    "#lookup if it is a ZCM Item \n",
    "print('5.- Reading ZCM Item Report')\n",
    "#df3 = pd.read_excel(\"ZCM Items.xlsx\", dtype={'SKU DC': str})\n",
    "#df3 = df3[['SKU DC','ZCM']]\n",
    "#df3.rename(columns={'SKU DC': 'SKU DC Key','ZCM':'TPC/ZCM Transmit'},inplace=True)\n",
    "df3 = pd.read_excel(\"ZCM Items.xlsx\", dtype={'Zebra PN': str,'Zebra Site':str})\n",
    "df3['SKU DC Key'] = df3['Zebra PN'] + df3['Zebra Site']\n",
    "df3['TPC/ZCM Transmit'] = \"Yes\"\n",
    "df3=df3[['SKU DC Key','TPC/ZCM Transmit']]\n",
    "df3=df3.drop_duplicates()\n",
    "df1 = pd.merge(df1, df3, on='SKU DC Key', how='left')\n",
    "df1['TPC/ZCM Transmit'].fillna('No', inplace = True)\n",
    "\n",
    "#lookup ASP\n",
    "print('6.- Reading Fill Rate')\n",
    "df4 = pd.read_excel(\"Fill Rate.xlsx\", dtype={'Part No': str, 'ASP_Original': float})\n",
    "df4 = df4[['Part No','ASP_Original']]\n",
    "df4.rename(columns={'Part No': 'PART','ASP_Original':'Part ASP $'},inplace=True)\n",
    "df4=df4.drop_duplicates(subset='PART',keep='first')\n",
    "df1 = pd.merge(df1, df4, on='PART', how='left')\n",
    "df1['Part ASP $'].fillna(0, inplace=True)\n",
    "df1.shape\n",
    "\n",
    "#Item Master printers\n",
    "print('7.- Reading Item Master printers')\n",
    "a=fnmatch.filter(os.listdir('.'), 'Item Master - Printers WK*')\n",
    "a=a[0]    \n",
    "dfIMP = pd.read_excel(a,header=0,dtype={'SKU DC': str,'Sourcing for':str},usecols=\"B,L\")\n",
    "dfIMP.rename(columns={'SKU DC': 'SKU DC Key'},inplace=True)\n",
    "df1 = pd.merge(df1,dfIMP, on='SKU DC Key', how='left')\n",
    "#df1['Sourcing for'] = np.where(df1['Sourcing for']== 'CTO', df1['Sourcing for'], '')#drop no cto\n",
    "df1['MATL_TYPE'] = np.where(df1['Sourcing for']== 'CTO', 'Simple Config', df1['MATL_TYPE']) #change cto to simple config \n",
    "df1 = df1.drop(['Sourcing for'], axis = 1)\n",
    "df1.shape\n",
    "\n",
    "#FINISH CTO Printers    \n",
    "print('8.- Reading Finish Printer CTO')\n",
    "a=fnmatch.filter(os.listdir('.'), 'Finish Printer - CTO Agile*')\n",
    "a=a[0] \n",
    "dfCTO = pd.read_excel(a,dtype={'SKU DC': str, 'Item Number (BOM)': str}, sheet_name='CTO')\n",
    "dfCTO = dfCTO[['SKU DC','Item Number (BOM)']]\n",
    "dfCTO=dfCTO.drop_duplicates(subset='SKU DC',keep='first')\n",
    "dfCTO.rename(columns={'SKU DC': 'SKU DC Key'},inplace=True)\n",
    "\n",
    "df1 = pd.merge(df1,dfCTO, on='SKU DC Key', how='left')\n",
    "#df1['Part ASP $'].fillna(0, inplace=True)\n",
    "df1['Config Printer SKU']=df1['Item Number (BOM)']\n",
    "df1 = df1.drop(['Item Number (BOM)'], axis = 1)\n",
    "\n",
    "#printers=pd.read_excel('Printers.xlsx')#AUXILIAR WITH YESTERDAYS INFO FROM AIT BL\n",
    "#df1 = pd.merge(df1,printers, on='SKU DC Key', how='left')\n",
    "print('9.- Reading ATS from yesterday file')\n",
    "a=fnmatch.filter(os.listdir('.'), 'ATS Global SP Backlog*')\n",
    "a=a[0] \n",
    "printers = pd.read_excel(a,dtype={'SKU DC Key': str, 'Config Printer SKU': str}, sheet_name='COMBINED',usecols='AA,AB',header=2)\n",
    "#print(printers.shape,'AIT YESTERDAY')\n",
    "#printers.head()\n",
    "\n",
    "printers = printers.dropna(subset = [\"Config Printer SKU\"]) #JUTS TAKE CONFIG PRINTER ROWS\n",
    "printers=printers.drop_duplicates() \n",
    "#print(printers.shape,'AIT YESTERDAY DISTINCT CONFIGS')\n",
    "\n",
    "df1 = pd.merge(df1,printers, on='SKU DC Key', how='left')\n",
    "\n",
    "#df1.loc[df1['SKU DC Key'] == \"R4D-0U0A000N-10TX2\"]\n",
    "df1['Config Printer SKU'] = df1['Config Printer SKU_x'].fillna(df1['Config Printer SKU_y'])\n",
    "#df1.loc[df1['SKU DC Key'] == \"R4D-0U0A000N-10TX2\"]\n",
    "df1['Config Printer SKU'] = np.where(df1['MATL_TYPE']== 'Simple Config', df1['Config Printer SKU'],'')\n",
    "#df1.loc[df1['SKU DC Key'] == \"R4D-0U0A000N-10TX2\"]\n",
    "\n",
    "df1 = df1.drop(['Config Printer SKU_x','Config Printer SKU_y'], axis = 1)\n",
    "\n",
    "#create an auxiliar table where Config Printer SKU is empty, to look up in Agile AIT\n",
    "dfSourcingRule=df1[df1['Config Printer SKU'].isnull()]\n",
    "dfSourcingRule=dfSourcingRule[['PART','Config Printer SKU']]\n",
    "dfSourcingRule=dfSourcingRule.drop_duplicates() \n",
    "dfSourcingRule.reset_index(inplace=True) #CREATE NEW INDEXES\n",
    "dfSourcingRule.drop(\"index\",axis=1,inplace=True)\n",
    "\n",
    "len(dfSourcingRule) #HOW MANY PARTS WE'LL LOOK FOR IN AGILE EVM\n",
    "for i in dfSourcingRule.index:\n",
    " a=dfSourcingRule['PART'][i]\n",
    " print (\"Look the next part number in Agile AIT\",{a})\n",
    " dfSourcingRule['Config Printer SKU'][i] = input()\n",
    "\n",
    "dfSourcingRule.rename(columns={'Config Printer SKU':'Config Printer SKU1'},inplace=True)\n",
    "df1 = pd.merge(df1, dfSourcingRule, on='PART', how='left')\n",
    "df1['Config Printer SKU'] = df1['Config Printer SKU'].fillna(df1['Config Printer SKU1'])\n",
    "\n",
    "df1 = df1.drop(['Config Printer SKU1'], axis = 1)\n",
    "df1['Config Printer SKU/DC'] = np.where(df1['MATL_TYPE']== 'Simple Config', df1['Config Printer SKU'] + df1['SHIP_LOC'], '')\n",
    "\n",
    "df1['Config Printer SKU/DC/WK Key'] = np.where(df1['MATL_TYPE']== 'Simple Config', df1['Config Printer SKU/DC'] + df1['CRSD WK'], '')\n",
    "\n",
    "#lookup Sourcing Rule, Buyer, Planner Code\n",
    "print('10.- Reading the Sourcing Rules for EVM')\n",
    "df5 = pd.read_csv('Sourcing Rule EVM.csv', dtype={'SKU DC': str})\n",
    "df5 = df5[['SKU DC','Item Organization - Sourcing Rule','Default Buyer','Planner Code']]\n",
    "df5.rename(columns={'SKU DC': 'SKU DC Key','Item Organization - Sourcing Rule':'Sourcing Rule',\n",
    "                          'Default Buyer':'BUYER.','Planner Code':'PLANNER CODE'},inplace=True)\n",
    "df1 = pd.merge(df1, df5, on='SKU DC Key', how='left')\n",
    "#df1['Sourcing Rule'].fillna(0, inplace=True)#left as empty to replace nulls with other data\n",
    "df1['BUYER.'].fillna(0, inplace=True)\n",
    "df1['PLANNER CODE'].fillna(0, inplace=True) \n",
    "\n",
    "#FILL NAs from Sourcing Rule WITH AGILE LIST\n",
    "print('11.- Reading Agile List')\n",
    "df6 = pd.read_excel(\"Agile List.xlsx\", dtype={'Item Number': str})\n",
    "df6 = df6[['Item Number','Manufacturer Name']]\n",
    "df6.rename(columns={'Item Number': 'PART'},inplace =True)\n",
    "df6=df6.drop_duplicates(subset='PART',keep='first') #some SO LINES have 2 dates, kept first as in excel\n",
    "df1 = pd.merge(df1, df6, on='PART', how='left')\n",
    "df1['Sourcing Rule'] = df1['Sourcing Rule'].fillna(df1['Manufacturer Name'])#where sourcing rule is empty\n",
    "#this values change to the result of the merge, ie the manufacturer name in agile file\n",
    "df1 = df1.drop(['Manufacturer Name'], axis = 1) #after gettin the values, column from agile is no longer needed\n",
    "#df1.shape\n",
    "\n",
    "#retake the sourcing rule file for Config Printer\n",
    "#df16=pd.read_csv('Sourcing Rule EVM.csv', dtype={'SKU DC': str})\n",
    "#df16 = df16[['SKU DC','Item Organization - Sourcing Rule']]\n",
    "df5.rename(columns={'Sourcing Rule':'Sourcing Rule ConfigP','SKU DC Key':'Config Printer SKU/DC'}, inplace = True)\n",
    "df5=df5[['Sourcing Rule ConfigP','Config Printer SKU/DC']]\n",
    "df1 = pd.merge(df1, df5, on='Config Printer SKU/DC', how='left')\n",
    "print(df1.shape)\n",
    "\n",
    "df1['Sourcing Rule'] = np.where(df1['MATL_TYPE']== 'Simple Config', df1['Sourcing Rule ConfigP'],df1['Sourcing Rule'])\n",
    "\n",
    "#create XFR column\n",
    "df1['XFR'] = np.where((df1['Sourcing Rule'].isnull()), '',\n",
    "             np.where(df1['Sourcing Rule'].str.contains('Transfer'), 'Yes',\n",
    "             np.where(df1['Sourcing Rule'].str.contains('transfer'), 'Yes', '')))\n",
    "\n",
    "#if Sourcing Rule starts with Make turn MATL_TYPE to bundle\n",
    "df1['MATL_TYPEaux'] = np.where(df1['Sourcing Rule'].astype(str).str.startswith('Make'), 'Bundle', df1['MATL_TYPE'])\n",
    "df1['MATL_TYPE'] = np.where(df1['MATL_TYPE']== 'Simple Config', df1['MATL_TYPE'], df1['MATL_TYPEaux'])\n",
    "df1 = df1.drop(['MATL_TYPEaux'], axis =1)\n",
    "#df1 = df1.drop(['Config Printer SKU1','Item Number (BOM)'], axis = 1)\n",
    "\n",
    "#if MATL_TYPE is Bundle make PART ASP $, UNIT_VAL, TOT_BL all zero\n",
    "df1['Part ASP $'] = np.where(df1['MATL_TYPE'].astype(str).str.startswith('Bundle'), 0, df1['Part ASP $'])\n",
    "df1['UNIT_VAL'] = np.where(df1['MATL_TYPE'].astype(str).str.startswith('Bundle'), 0, df1['UNIT_VAL'])\n",
    "df1['TOT_BL'] = np.where(df1['MATL_TYPE'].astype(str).str.startswith('Bundle'), 0, df1['TOT_BL'])\n",
    "\n",
    "#create Blended ASP and EXT Part ASP\n",
    "df1['Blended ASP $'] = np.where((df1['UNIT_VAL'] > 0), df1['UNIT_VAL'], df1['Part ASP $'])\n",
    "df1['EXT Part ASP $'] = df1['ACTUAL_QTY'] * df1['Blended ASP $']\n",
    "\n",
    "#AUXILIAR COLUMN, WHEN SIMPLE CONFIG WELL HAVE THE CONFIG SKUDC VALUE, WHEN NOT, WELL HAVE SKU DC VALUE\n",
    "df1['aux sku dc'] = np.where(df1['MATL_TYPE'] == 'Simple Config', df1['Config Printer SKU/DC'],df1['SKU DC Key'])\n",
    "#print(df1.shape)\n",
    "\n",
    "#lookup Oracle Intransit(Trilogy=>Merlin or Merlin=>Trilogy)\n",
    "print('12.- Reading Inter-Org Intransit')\n",
    "df7 = pd.read_csv(\"Inter-Org Intransit.csv\", dtype={'ITEM': str})\n",
    "df7['aux sku dc'] = df7['ITEM'] + df7['ORG'].str[4:]\n",
    "df7 = pd.pivot_table(df7, values=['DEMAND_SUPPLY_QTY'], index=['aux sku dc'], aggfunc=np.sum)\n",
    "df1 = pd.merge(df1, df7, on='aux sku dc', how='left')\n",
    "df1['Oracle Intransit(Trilogy=>Merlin or Merlin=>Trilogy)'] = df1['DEMAND_SUPPLY_QTY']\n",
    "df1['Oracle Intransit(Trilogy=>Merlin or Merlin=>Trilogy)'].fillna(0, inplace=True)\n",
    "df1 = df1.drop(['DEMAND_SUPPLY_QTY'], axis = 1)\n",
    "#print(df1.shape)\n",
    "\n",
    "#Merge shipment files together\n",
    "import glob\n",
    "print(\"13.- Reading the shipments\")\n",
    "path = r'C:\\Users\\MJ7255\\Desktop\\Backlog\\INSUMOS ASN' # use your path\n",
    "all_files = glob.glob(path + \"/*.xlsx\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_excel(filename, index_col=None, header=1)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "#export ASN file\n",
    "from datetime import date\n",
    "#today = date.today()#tday = str(today.strftime(\"%m/%d/%y\")) #tday='ASNs ' + tday #today\n",
    "#frame.to_excel('ASNs '+str(date.today())+'.xlsx', index=False)\n",
    "frame=frame[['*Zebra PN','*Ship To Site','*Shipped Quantity']]\n",
    "frame['aux sku dc']=frame[\"*Zebra PN\"] + frame[\"*Ship To Site\"]\n",
    "df11 = pd.pivot_table(frame, values=['*Shipped Quantity'], index=['aux sku dc'], aggfunc=np.sum)\n",
    "df1 = pd.merge(df1, df11, on='aux sku dc', how='left')\n",
    "df1['ZCM ASN QTY Supply'] = df1['*Shipped Quantity']\n",
    "df1['ZCM ASN QTY Supply'].fillna(0, inplace=True)\n",
    "df1 = df1.drop(['*Shipped Quantity'], axis = 1)\n",
    "#df11.to_excel('ASN PIVOT.xlsx')\n",
    "#print(\"ASN file created\")\n",
    "#print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14.- Reading All Commit_ZCM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MJ7255\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4379: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n",
      "<ipython-input-2-9f769ad25727>:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfCB.dropna(subset = [\"Config Printer SKU/DC\"], inplace=True) #JUTS TAKE CONFIG PRINTER ROWS\n",
      "<ipython-input-2-9f769ad25727>:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfCB['QOH_2'] = dfCB['QOH']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.-Reading AIT WORK ORDERS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MJ7255\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4379: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n",
      "<ipython-input-2-9f769ad25727>:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfwopiv1.dropna(subset = [\"Config Printer SKU/DC\"], inplace=True)#JUTS TAKE CONFIG PRINTER ROWS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.Reading Printers Inventory On Hand & Safety Stock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-9f769ad25727>:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df15simple['QOH2'] = df15simple['QOH']\n",
      "<ipython-input-2-9f769ad25727>:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df15config['QOH SimpleConfig'] = df15config['QOH (Part Simple Config + BASE+ WO Supply)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file\n",
      "--- 34.77385207017263 minutes to create AIT BL---\n"
     ]
    }
   ],
   "source": [
    "#lookup weekly ZCM Commits\n",
    "print('14/14.- Reading All Commit_ZCM')\n",
    "a=fnmatch.filter(os.listdir('.'), 'All Commit_ZCM*')\n",
    "a=a[0]\n",
    "\n",
    "df12 = pd.read_excel(a, header=1, sheet_name='ZCM COMMITS', dtype={'SKU DC': str})\n",
    "df12 = df12[(df12['Supplier ID'] != 'Supplier ID')] #subset rows with info, there some rows with wronf information, headers in ros and dates in week info\n",
    "df12 = df12.rename(columns={'SKU DC': 'aux sku dc'})\n",
    "df12 = df12[['aux sku dc','ALL MISSING','ALL WK 1 & 2','Data Measure','Week 1','Week 2','Week 3','Week 4','Week 5']]\n",
    "\n",
    "df12 = df12.groupby(['aux sku dc']).sum()\n",
    "df12.sort_values(by='ALL MISSING', ascending=False, inplace=True)\n",
    "\n",
    "df1 = pd.merge(df1, df12, on='aux sku dc', how='left')\n",
    "\n",
    "df1['ZCM Commits WK1'] = df1['Week 1']\n",
    "df1['ZCM Commits WK1'].fillna(0, inplace=True)\n",
    "df1['ZCM Commits WK2'] = df1['Week 2']\n",
    "df1['ZCM Commits WK2'].fillna(0, inplace=True)\n",
    "df1['ZCM Commits WK3'] = df1['Week 3']\n",
    "df1['ZCM Commits WK3'].fillna(0, inplace=True)\n",
    "df1['ZCM Commits WK4'] = df1['Week 4']\n",
    "df1['ZCM Commits WK4'].fillna(0, inplace=True)\n",
    "df1['ZCM Commits WK5'] = df1['Week 5']\n",
    "df1['ZCM Commits WK5'].fillna(0, inplace=True)\n",
    "df1 = df1.drop(['ALL MISSING','Week 1','Week 2','Week 3','Week 4','Week 5'], axis = 1)\n",
    "\n",
    "#print(df1.shape, 'numer of rows and columns in COMBINED tab')\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "dfCB=df1[['SKU DC Key','QOH','Config Printer SKU/DC']]\n",
    "#print(dfCB.shape,\"should be same numer of rows with 3 columns\")\n",
    "dfCB.replace(\"\", nan_value, inplace=True)\n",
    "dfCB.dropna(subset = [\"Config Printer SKU/DC\"], inplace=True) #JUTS TAKE CONFIG PRINTER ROWS\n",
    "#print(dfCB.shape,\"number of rows that are Simple Config\")\n",
    "#dfCB.head()\n",
    "\n",
    "#CREATE SKU DC PIV SHEET\n",
    "dfCB['QOH_2'] = dfCB['QOH']\n",
    "\n",
    "dfCBpiv = pd.pivot_table(data=dfCB, index=['SKU DC Key','Config Printer SKU/DC'], values=['QOH','QOH_2'], aggfunc={'QOH': 'sum', 'QOH_2': 'count'}).reset_index()\n",
    "#generate new column of piv tab\n",
    "\n",
    "dfCBpiv['Org Part INV-Part Simple Config Total QOH'] = dfCBpiv['QOH'] / dfCBpiv['QOH_2']\n",
    "#dfCBpiv.head()\n",
    "\n",
    "dfCBpiv=dfCBpiv[['Config Printer SKU/DC','Org Part INV-Part Simple Config Total QOH']] #just take columns needed\n",
    "#print(dfCBpiv.shape, \"several SKU DC with same config sku dc\")\n",
    "#dfCBpiv = dfCBpiv.drop_duplicates(subset = 'Config Printer SKU/DC',keep='first') done before\n",
    "#bool_series = pd.notnull(dfCBpiv[\"Config Printer SKU/DC\"])\n",
    "#dfCBpiv=dfCBpiv[bool_series]\n",
    "#dfCBpiv=dfCBpiv.drop_duplicates()\n",
    "#print(dfCBpiv.shape, \"Unique Config Printer SKU, first rows kept\")\n",
    "#dfCBpiv.head()\n",
    "\n",
    "dfCBpiv2 = dfCBpiv.groupby(['Config Printer SKU/DC'],as_index = False).sum()\n",
    "#print(dfCBpiv2.shape, 'final pivot for  1st column')\n",
    "#dfCBpiv2.head()\n",
    "\n",
    "df1 = pd.merge(df1, dfCBpiv2, on='Config Printer SKU/DC', how='left') #first yellow printer column\n",
    "#print(df1.shape, \"Dimensions after merge of 1st piv\")\n",
    "df1['Org Part INV-Part Simple Config Total QOH']=np.where(df1['MATL_TYPE'] == 'Simple Config', df1['Org Part INV-Part Simple Config Total QOH'],0)\n",
    "df1['Org Part INV-Part Simple Config Total QOH']=pd.to_numeric(df1['Org Part INV-Part Simple Config Total QOH'])\n",
    "\n",
    "# CREATE CB TO CE COLUMNS IN COMBINED\n",
    "## TO CREATE A UNIFIED QOH AND THEN CREATE COMBINED BASE+COMBINED IN JUST ONE PIVOT, ASN, INTRANSIT AND COMMITSBY WEEKS \n",
    "#ARE ALREADY GENERATED\n",
    "\n",
    "#READ WORK ORDERS pivot\n",
    "print('15.-Reading AIT WORK ORDERS')\n",
    "a=fnmatch.filter(os.listdir('.'), 'AIT WORK ORDERS*')\n",
    "a=a[0]\n",
    "dfWO = pd.read_excel(a, header=3, sheet_name='WORK ORDER', dtype={'SKU DC': str}, usecols='N,O')\n",
    "#MERGE WITH WO PIVOT\n",
    "dfwopiv1=df1[['SKU DC Key','Config SKU/DC WO Supply Simple Config','Config Printer SKU/DC']]\n",
    "#print(dfwopiv1.shape, 'data to make WO pivot')\n",
    "\n",
    "dfwopiv1.replace(\"\", nan_value, inplace=True)\n",
    "dfwopiv1.dropna(subset = [\"Config Printer SKU/DC\"], inplace=True)#JUTS TAKE CONFIG PRINTER ROWS\n",
    "#print(dfwopiv1.shape, 'number of rows that are Simple Config' )\n",
    "\n",
    "#CREATE SKU DC PIV WO SHEET\n",
    "dfwopiv = pd.pivot_table(data=dfwopiv1, index=['SKU DC Key'], values=['Config SKU/DC WO Supply Simple Config'], aggfunc={'Config SKU/DC WO Supply Simple Config': 'count'}).reset_index()\n",
    "#dfwopiv.head()\n",
    "#dfWO.rename(columns={'SKU DC':'SKU DC Key'}, inplace=True)\n",
    "\n",
    "dfwopiv = pd.merge(dfwopiv, dfWO, on='SKU DC Key', how='left')\n",
    "dfwopiv.replace(nan_value, 0,inplace=True)\n",
    "\n",
    "dfwopiv['NWO'] = dfwopiv['Sum of Quantity'] / dfwopiv['Config SKU/DC WO Supply Simple Config']\n",
    "#dfwopiv.head()\n",
    "\n",
    "dfwopiv=dfwopiv[['SKU DC Key','NWO']]\n",
    "#dfwopiv.head()\n",
    "\n",
    "#print(df1.shape,'Dimensions after PIV tab, before joining WO tab')\n",
    "\n",
    "df1 = pd.merge(df1, dfwopiv, on='SKU DC Key', how='left') #second yellow printer column\n",
    "df1['Config SKU/DC WO Supply Simple Config']=np.where(df1['MATL_TYPE'] == 'Simple Config', df1['NWO'],0)\n",
    "df1['Config SKU/DC WO Supply Simple Config'] = pd.to_numeric(df1['Config SKU/DC WO Supply Simple Config'])\n",
    "#dfwopiv.head()\n",
    "#print(df1.shape,'Dimensions after joining WO tab')\n",
    "\n",
    "#3drd yellow columnlookup PRINTERS NETTABLE AVILABLE QTY\n",
    "print('16.Reading Printers Inventory On Hand & Safety Stock')\n",
    "df22 = pd.read_csv('Printers Inventory On Hand & Safety Stock.csv')\n",
    "df22 = df22[['SKU DC','Nettable Available Quantity']] #just columns needed\n",
    "df22.rename(columns={'SKU DC':'Config Printer SKU/DC','Nettable Available Quantity':'Config SKU/DC INV-Simple Config'}, inplace=True)\n",
    "df1 = pd.merge(df1, df22, on='Config Printer SKU/DC', how='left')\n",
    "df1['Config SKU/DC INV-Simple Config'] = df1['Config SKU/DC INV-Simple Config'].fillna(0)\n",
    "#print(df1.shape,'Dimensions after joining Printers Inventory On Hand')\n",
    "\n",
    "#sum of previous yellow columns, last yellow column\n",
    "#df1['Org Part INV-Part Simple Config Total QOH']=np.where(df1['MATL_TYPE'] == 'Simple Config', df1['Org Part INV-Part Simple Config Total QOH'],0)\n",
    "df1['Config SKU/DC INV-Simple Config'] = df1['Config SKU/DC INV-Simple Config'].fillna(0)\n",
    "df1['Config SKU/DC WO Supply Simple Config'] = pd.to_numeric(df1['Config SKU/DC WO Supply Simple Config'])\n",
    "df1['Config SKU/DC INV-Simple Config'] = pd.to_numeric(df1['Config SKU/DC INV-Simple Config'])\n",
    "df1['QOH (Part Simple Config + BASE+ WO Supply)']= df1['Org Part INV-Part Simple Config Total QOH'] + df1['Config SKU/DC WO Supply Simple Config'] + df1['Config SKU/DC INV-Simple Config']\n",
    "#print(df1.shape,'after sum of yellow columns')\n",
    "\n",
    "#QOH (Part Simple Config + BASE+ WO Supply)\n",
    "\n",
    "#AUXILIAR COLUMN, FOR QOH WHEN SIMPLE CONFIG WELL HAVE THE RESULT OF PIVOTS, WHEN SIMPLE, WELL HAVE QOH\n",
    "#df1['auxQOH'] = np.where(df1['MATL_TYPE'] == 'Simple Config', df1['QOH (Part Simple Config + BASE+ WO Supply)'],df1['QOH'])\n",
    "\n",
    "#create Short(QTYs On Hand) DBL & CRSD <=+4 columns\n",
    "#use auxiliar for skudc & qoh\n",
    "df15 = df1[['SHIP_LOC','Trilogy Or Merlin','CRSD','SO','LINE','SO_LINE','SKU DC Key', 'Config Printer SKU/DC',\n",
    "            'MATL_TYPE','ACTUAL_QTY','QOH', 'QOH (Part Simple Config + BASE+ WO Supply)']]\n",
    "\n",
    "#auxiliar of dates to take within 4 days\n",
    "today = date.today()\n",
    "four_days = today + datetime.timedelta(days=3)\n",
    "four_days = four_days.strftime('%Y-%m-%d')\n",
    "#when is less than five days\n",
    "df15 = df15[df15['CRSD'] <= four_days]\n",
    "#df15.head()\n",
    "\n",
    "#Select SKU Short(QTYs On Hand) DBL & CRSD <=+4 simple\n",
    "df15simple = df15[df15['MATL_TYPE'] == 'Simple']\n",
    "df15simple['QOH2'] = df15simple['QOH']\n",
    "df15simple.reset_index(inplace = True)\n",
    "df15simple=df15simple.drop(['QOH (Part Simple Config + BASE+ WO Supply)','Config Printer SKU/DC','index'], axis = 1)\n",
    "\n",
    "#Select SKU Short(QTYs On Hand) DBL & CRSD <=+4 simple config\n",
    "df15config = df15[df15['MATL_TYPE'] == 'Simple Config']\n",
    "df15config['QOH SimpleConfig'] = df15config['QOH (Part Simple Config + BASE+ WO Supply)']\n",
    "df15config.reset_index(inplace = True)\n",
    "df15config=df15config.drop(['QOH','SKU DC Key','index'],axis = 1)\n",
    "#df15config.to_excel('DBLCONGIFP.xlsx', index = False)\n",
    "\n",
    "#create SKU Short(QTYs On Hand) DBL & CRSD <=+4 PIVOT\n",
    "df16simple = pd.pivot_table(data=df15simple, index=['SKU DC Key'], values=['QOH','QOH2','ACTUAL_QTY'], aggfunc={'QOH': 'sum', 'QOH2': 'count', 'ACTUAL_QTY': 'sum'}).reset_index()\n",
    "df16simple['SKU/DC QOH'] = df16simple['QOH'] / df16simple['QOH2']\n",
    "df16simple['Short +/-'] = df16simple['SKU/DC QOH'] - df16simple['ACTUAL_QTY']\n",
    "df16simple['SKU Short'] = np.where(df16simple['Short +/-'] <= 0, 'YES', 'NO')\n",
    "\n",
    "#create SKU Short(QTYs On Hand) DBL & CRSD <=+4 PIVOT\n",
    "df16config = pd.pivot_table(data=df15config, index=['Config Printer SKU/DC'], values=['QOH (Part Simple Config + BASE+ WO Supply)','QOH SimpleConfig','ACTUAL_QTY'], aggfunc={'QOH (Part Simple Config + BASE+ WO Supply)': 'sum', 'QOH SimpleConfig': 'count', 'ACTUAL_QTY': 'sum'}).reset_index()\n",
    "df16config['SKU/DC QOH'] = df16config['QOH (Part Simple Config + BASE+ WO Supply)'] / df16config['QOH SimpleConfig']\n",
    "df16config['Short +/-'] = df16config['SKU/DC QOH'] - df16config['ACTUAL_QTY']\n",
    "df16config['SKU Short'] = np.where(df16config['Short +/-'] <= 0, 'YES', 'NO')\n",
    "\n",
    "#df16simple.to_excel('1stpivotsimple.xlsx', index = False)\n",
    "#df16config.to_excel('DBL_CRSD4daysSKUConfigP.xlsx', index = False)\n",
    "df16simple = df16simple[['SKU DC Key','SKU Short']]\n",
    "df16config = df16config [['Config Printer SKU/DC','SKU Short']]\n",
    "#print('1ST PIVOT FOR BOTH SIMPLE AND SIMPLE CONFIG')\n",
    "#df16.head()\n",
    "\n",
    "#print(df1.shape,'DIMENSIONS OF main tab before join PIV<4 DIAS for 1st column')\n",
    "df1 = pd.merge(df1, df16simple, on='SKU DC Key', how='left')\n",
    "#print(df1.shape,'DIMENSIONS OF main tab after join PIV<4 DIAS for first column')\n",
    "df1['SKU Short(QTYs On Hand) DBL & CRSD <=+4'] = df1['SKU Short']\n",
    "df1 = df1.drop(['SKU Short'], axis = 1)\n",
    "\n",
    "df1['SKU Short(QTYs On Hand) DBL & CRSD <=+4'] = np.where( (df1['CRSD'] <= four_days) & (df1['MATL_TYPE'] == 'Simple'), df1['SKU Short(QTYs On Hand) DBL & CRSD <=+4'], '')\n",
    "#print('Pivot tab DBL & CRSD <=+4 dimensions version 1, before 2nd column S')\n",
    "\n",
    "#Create SO Short(QTYs On Hand) DBL & CRSD <=+4 PIVOT\n",
    "df15simple = pd.merge(df15simple, df16simple, on='SKU DC Key', how='left')\n",
    "#print('Pivot tab DBL & CRSD <=+4 dimensions final version', df15simple.shape)\n",
    "#df15.head()\n",
    "\n",
    "#print(df1.shape,'DIMENSIONS OF main tab before join PIV<4 DIAS for 1st column simple config')\n",
    "df1 = pd.merge(df1, df16config, on='Config Printer SKU/DC', how='left')\n",
    "#print(df1.shape,'DIMENSIONS OF main tab after join PIV<4 DIAS for first column')\n",
    "df1['SKU Short(QTYs On Hand) DBL & CRSD <=+4'] = np.where((df1['CRSD'] <= four_days) & (df1['MATL_TYPE'] == 'Simple Config'), \n",
    "                                                          df1['SKU Short'], df1['SKU Short(QTYs On Hand) DBL & CRSD <=+4'])\n",
    "df1 = df1.drop(['SKU Short'], axis = 1)\n",
    "#print('Pivot tab DBL & CRSD <=+4 dimensions version 1, before 2nd column S')\n",
    "\n",
    "#Create SO Short(QTYs On Hand) DBL & CRSD <=+4 PIVOT\n",
    "df15config = pd.merge(df15config, df16config, on='Config Printer SKU/DC', how='left')\n",
    "#print('Pivot tab DBL & CRSD <=+4 dimensions final version config', df15config.shape)\n",
    "#df15.head()\n",
    "\n",
    "df17simple = pd.crosstab(index=df15simple[\"SO\"], columns=df15simple[\"SKU Short\"], \n",
    "                         values=df15simple[\"SKU DC Key\"], aggfunc='count').reset_index()\n",
    "\n",
    "df17config = pd.crosstab(index=df15config[\"SO\"], columns=df15config[\"SKU Short\"], \n",
    "                         values=df15config[\"Config Printer SKU/DC\"], aggfunc='count').reset_index()\n",
    "#df17config.head()\n",
    "\n",
    "#print(df1.shape,'before joining piv simple')\n",
    "df17simple['SO Short'] = np.where(df17simple['YES'].isnull(), 'NO', 'YES')\n",
    "\n",
    "df17simple = df17simple[['SO','SO Short']]\n",
    "df1 = pd.merge(df1, df17simple, on='SO', how='left')\n",
    "#print(df1.shape,'after joining piv simple')\n",
    "df17config['SO Shortconfig'] = np.where(df17config['YES'].isnull(), 'NO', 'YES')\n",
    "df17config = df17config[['SO','SO Shortconfig']]\n",
    "df1 = pd.merge(df1, df17config, on='SO', how='left')\n",
    "#print(df1.shape, 'after joining piv simple config')\n",
    "\n",
    "df1['SO Short(QTYs On Hand) DBL & CRSD <=+4'] = np.where((df1['CRSD'] <= four_days) & (df1['MATL_TYPE'] == 'Simple'), df1['SO Short'], \n",
    "                                                np.where((df1['CRSD'] <= four_days) & (df1['MATL_TYPE'] == 'Simple Config'), df1['SO Shortconfig'], ''))\n",
    "\n",
    "#df1 = df1.drop(['SO Short','SO Shortconfig'], axis = 1)\n",
    "\n",
    "#print(df1.shape, 'after dropping so shorts and assigning proper ')\n",
    "df1['DATE'] = pd.to_datetime(df1['DATE'])\n",
    "\n",
    "#df1['day_of_wk'] = \"Wednesday\"\n",
    "\n",
    "#create ZCM ASN/Commit Net WK1 supply\n",
    "df1['day_of_wk'] = df1['DATE'].dt.day_name()\n",
    "\n",
    "df1['ZCM ASN/Commit Net WK1 Supply'] = np.where(df1['day_of_wk'] == 'Wednesday', df1['ZCM ASN QTY Supply'] + df1['ZCM Commits WK1'],\n",
    "                                       np.where(df1['day_of_wk'] == 'Thursday', df1['ZCM ASN QTY Supply'] + df1['ZCM Commits WK1'],\n",
    "                                       np.where(df1['day_of_wk'] == 'Friday', df1['ZCM ASN QTY Supply'] + df1['ZCM Commits WK1'],\n",
    "                                       np.where(df1['day_of_wk'] == 'Saturday', df1['ZCM ASN QTY Supply'] + df1['ZCM Commits WK1'],\n",
    "                                       np.where(df1['day_of_wk'] == 'Sunday', df1['ZCM ASN QTY Supply'] + df1['ZCM Commits WK1'],\n",
    "                                       np.where((df1['day_of_wk'] == 'Monday') & (df1['ZCM ASN QTY Supply'] >= df1['ZCM Commits WK1']), df1['ZCM ASN QTY Supply'],\n",
    "                                       np.where((df1['day_of_wk'] == 'Tuesday') & (df1['ZCM ASN QTY Supply'] >= df1['ZCM Commits WK1']), df1['ZCM ASN QTY Supply'], df1['ZCM Commits WK1'])))))))\n",
    "df1 = df1.drop(['day_of_wk'], axis = 1)\n",
    "\n",
    "#print(df1.shape)\n",
    "\n",
    "## 2ND check POINT divide simple vs simple config\n",
    "#CREATE RED COLUMN for simple\n",
    "#THE SKU DC WITH DIFERENT COMMENTS WILL KEEP ONLY ONE, IF SEVERAL THE WILL BE CONCATANETED\n",
    "#Select SKU Short(QTYs On Hand) DBL & CRSD <=+4 simple config\n",
    "#df15config = df15[df15['MATL_TYPE'] == 'Simple Config']\n",
    "df13simple=df1[df1['MATL_TYPE'] != 'Simple Config']\n",
    "df13 = pd.crosstab(index=df13simple[\"SKU DC Key\"], columns=df13simple[\"CRSD WK\"], values=df13simple[\"CRSD WK\"], aggfunc='count').fillna(0)\n",
    "df13 = df13.reset_index()\n",
    "df13.sort_values(by='SKU DC Key', ascending=True, inplace=True)\n",
    "\n",
    "df13['WK1'] = df13['WK1'].astype(int)\n",
    "df13['WK2'] = df13['WK2'].astype(int)\n",
    "df13['WK3'] = df13['WK3'].astype(int)\n",
    "#df13.to_excel('RED COLUMN TABLE1.xlsx', index=False)\n",
    "df13['WK1'] = np.where(df13['WK1'] > 0, 'Yes', 'No')\n",
    "df13['WK2'] = np.where(df13['WK2'] > 0, 'Yes', 'No')\n",
    "df13['WK3'] = np.where(df13['WK3'] > 0, 'Yes', 'No')\n",
    "df13[''] = np.where(df13[''] > 0, 'Yes', 'No')\n",
    "\n",
    "#df13 = df13.drop([''], axis = 1)\n",
    "df13['Missing CRSD Wksimple'] = np.where((df13['WK1'] == 'No') & (df13['WK2'] == 'No') & (df13['WK3'] == 'No'), 'All Missing Blank',\n",
    "                          np.where((df13['WK1'] == 'No') & (df13['WK2'] == 'Yes') & (df13['WK3'] == 'No') & (df13[''] == 'Yes'), 'Missing WK1,Missing WK3',#\n",
    "                          np.where((df13['WK1'] == 'No') & (df13['WK2'] == 'No') & (df13['WK3'] == 'Yes'), 'All Missing WK3',\n",
    "                          np.where((df13[''] == 'Yes') & (df13['WK2'] == 'Yes') & (df13['WK3'] == 'No'), 'Missing WK3',         \n",
    "                          np.where((df13['WK1'] == 'Yes') & (df13['WK3'] == 'Yes') & (df13['WK2'] == 'No'), 'Missing WK2',\n",
    "                          np.where((df13['WK2'] == 'Yes') & (df13['WK1'] == 'No'), 'Missing WK1', 'Nothing Missing'))))))\n",
    "#df13.to_excel('RED COLUMN TABLEsimple.xlsx', index=False)\n",
    "#print(\"Red Column for simple created\")\n",
    "df13simple = pd.merge(df13simple, df13, on='SKU DC Key', how='left')\n",
    "#df1 = df1.drop(['WK1','WK2','WK3'], axis = 1)\n",
    "\n",
    "df13simple['DISTY']=''\n",
    "df13simple['SLSOFF2']=''\n",
    "df13simple['PSION']=''\n",
    "df13simple['COMPLETE']=''\n",
    "df13simple['IR_NUMBER']=''\n",
    "#print('Reordering columns')\n",
    "df13simple = df13simple[[\"DATE\",\"THEATRE\",\"SHIP_LOC\",\"Trilogy Or Merlin\",\"ORD_TYPE\",\"SLSOFF2\",\"SLSOFF2_DE\",\"BOOKED\",\"CRSD\",\n",
    "           \"Material Avail Date > CRSD\",\"SSD\",\"FSSD\",\"DFF_CRSD\",\"SO\",\"LINE\",\"SO_LINE\",\"SO Line Dropped to DC for Delivery\",\n",
    "           \"SKU Short(QTYs On Hand) DBL & CRSD <=+4\",\"SO Short(QTYs On Hand) DBL & CRSD <=+4\",\n",
    "           \"SHIP-SET $'s\",\"ORDER $'s\",\"SHIP_SET\",\"PART\",\"Missing CRSD Wksimple\",\"CRSD WK\",\"TPC SKU/DC/WK Key\",\"SKU DC Key\",\n",
    "           \"Config Printer SKU\",\"Config Printer SKU/DC\",\"Config Printer SKU/DC/WK Key\",\"TPC/ZCM Transmit\",\"MATL_TYPE\",\n",
    "           \"ACTUAL_QTY\",\"REMAIN_QTY\",\"Part ASP $\",\"Blended ASP $\",\n",
    "           \"EXT Part ASP $\",\"UNIT_VAL\",\"TOT_BL\",\"TOP-FILE $'s\",\"SOLDTO_NO\",\"SOLD_TO\",\"SHIP_TO\",\"SHIP_DESC\",\"HOLD\",\n",
    "           \"DISTY\",\"DISTY_1\",\"TOTAL_ORD\",\"FAMILY\",\"PROD_LINE\",\"CORE\",\"COMPLETE\",\"PO\",\"SOURCE\",\"Sourcing Rule\",\"XFR\",\"SOURCE_2\",\n",
    "           \"STATUS\",\"CARRIER\",\"SHIP_MTHD\",\"IR_NUMBER\",\"DAYS_OLD\",\"DAYS_OLD2\",\"BUYER.\",\"PLANNER CODE\",\"BUYER\",\"INVC_QTY\",\n",
    "           \"REMAIN_INV\",\"DISTY2\",\"FSSD_CRSD\",\"HOLD_2\",\"REASON\",\"SHIP_QTY\",\"Org Part INV-Part Simple Config Total QOH\",\n",
    "           \"Config SKU/DC WO Supply Simple Config\",\"Config SKU/DC INV-Simple Config\",\"QOH (Part Simple Config + BASE+ WO Supply)\",\n",
    "           \"QOH\",\"Oracle Intransit(Trilogy IR/ISO)\",\"Oracle Intransit(Trilogy=>Merlin or Merlin=>Trilogy)\",\"ZCM ASN QTY Supply\",\n",
    "           \"ZCM Commits WK1\",\"ZCM Commits WK2\",\"ZCM Commits WK3\",\"ZCM Commits WK4\",\"ZCM Commits WK5\",\"WK1 TPC Receipt QTY\",\n",
    "           \"Valid TPC Supply\",\"TPC Commits WK1\",\"TPC Commits WK2\",\"TPC Commits WK3\",\"TPC Commits WK4\",\"ZCM ASN/Commit Net WK1 Supply\",\n",
    "           \"ZCM ASN QTY and WK1 Net Supply\",\"ZCM Net Commits WK2\",\"ZCM Net Commits WK3\",\"Valid TPC Net Supply WK1\",\n",
    "           \"TPC Net Commits WK2\",\"TPC Net Commits WK3\",\"Missing Commits TPC/ZCM WK2\",\"Net Supply capture by CRSD SKU/DC/WK\",\n",
    "           \"Missing ASN Supply Due to CRSD WK\",\"Missing Commit Supply Due to CRSD WK\",\"Merlin/Trilogy Missing Supply Total\",\n",
    "           \"ORG_BOOK\",\"SKU/DC Total Supply(OH only)\",\"QTY Short to Sales Orders(CUM) Only INV\",\"QTY Short to Sales Orders(NON-CUM) Only INV\",\n",
    "           \"CRSD_WEEK\",\"CRSD_MONTH\",\"CRSD_QTR\",\"SSD_WEEK\",\"SSD_MONTH\",\"SSD_QTR\",\"PSION\",\"COUNTRY\",\"DELINQ\",\"ALIGN\",\"BOOK_WEEK\",\n",
    "           \"BU\",\"SYSTEM\",\"CREATED_BY\",\"DFF_CRSD2\",\"BOOK_CRSD\",\"BOOK_CRSD2\",\"DAYS_OLD3\",\"SUB_FAMILY\",\"CODE_NAME\",\"THEATRE1\",\n",
    "           \"EVM_AIT\",\"HOLD_3\",\"INCO\",\"EVM_AIT2\",\"DELINQ_SSD\",\"DAYS_OLD4\",\"PAY_TERMS\",\"FISC_ZCRD\",\"FISC_SSD\",\"BUSINESS\",\"PRIORITY\",\n",
    "           \"STATE\",\"SUB_LINE\",\"EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM) Only INV)\",\"SKU/DC Total Supply(On Hand+ALL ASN)\",\n",
    "           \"QTY Short to Sales Orders(CUM) (On Hand+ALL ASN\",\"QTY Short to Sales Orders(NON-CUM) On Hand+ALL ASN\",\n",
    "           \"EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM)) On Hand+ALL ASN\",\"SKU/DC Total Supply ALL ASNs+Commits(No Missing Commits)\",\n",
    "           \"QTY Short to Sales Orders(CUM) including Missing Commits+Commits+ALL ASNs\",\"QTY Short to Sales Orders(NON-CUM) w/ Commits+Missing Commits+ALL ASNs\",\n",
    "           \"EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM)) INV w/ Commits+Missing Commits+ALL ASNs\",\"SKU/DC Total Supply(OH+ALL ASNs+all 3wk Commtsh)\",\n",
    "           \"QTY Short to Sales Orders(CUM) OH+ALL ASNs+all 3wk Commts\",\"QTY Short to Sales Orders(NON-CUM) OH+ALL ASNs+all 3wk Commts\",\n",
    "           \"EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM)) OH+ALL ASNs+all 3wk Commts\"]]\n",
    "\n",
    "df13simple.sort_values(by=['SKU DC Key','CRSD'],inplace=True)\n",
    "\n",
    "#DE\n",
    "#Auxiliar to keep only first rows, it is false in AUX1, AUX1 answers to the question, is this a duplicate row?\n",
    "df13simple['skuduplicates']=df13simple.duplicated(subset=['SKU DC Key']) #FALSE FOR FIRST APPEARENCE of SKU DC Key,TRUE FOR DUPLICATES\n",
    "#DUPLICATES FOR SKUDC+WK\n",
    "df13simple['skuduplicates2']=df13simple.duplicated(subset=['TPC SKU/DC/WK Key'])\n",
    "#df1 = pd.merge(df1, df13simple, on='Config Printer SKU/DC', how='left')\n",
    "\n",
    "\n",
    "df13simple['AUX'] = np.where((df13simple['Missing CRSD Wksimple'] == 'Missing WK1,Missing WK3') & (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK']== 'WK2'), \n",
    "                            df13simple['ZCM Commits WK1'],\n",
    "             np.where((df13simple['Missing CRSD Wksimple'] == 'All Missing WK3') & (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK']== 'WK3'),\n",
    "                      df13simple['ZCM Commits WK1'] + df13simple['ZCM Commits WK2'],\n",
    "             np.where((df13simple['Missing CRSD Wksimple'] == 'All Missing Blank') & (df13simple['skuduplicates']== False),\n",
    "                      df13simple['ZCM Commits WK1'] + df13simple['ZCM Commits WK2'] + df13simple['ZCM Commits WK3'],\n",
    "             np.where((df13simple['Missing CRSD Wksimple'] == 'Missing WK1') & (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK']== 'WK2'),\n",
    "                      df13simple['ZCM Commits WK1'],\n",
    "             np.where((df13simple['Missing CRSD Wksimple'] == 'Missing WK2') & (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK']== 'WK3'),\n",
    "                      df13simple['ZCM Commits WK2'],\n",
    "             np.where((df13simple['Missing CRSD Wksimple'] == 'Missing WK3') & (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK']== ''),\n",
    "                      df13simple['ZCM Commits WK3'],\n",
    "            np.where((df13simple['Missing CRSD Wksimple'] == 'Missing WK1,Missing WK3') & (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK']== ''),\n",
    "                      df13simple['ZCM Commits WK3'],0)))))))\n",
    "#COLUMN ED MUTIPLE VLOOKUPS DEPENDING ON RED COLUMN\n",
    "df13simple['Missing Commit Supply Due to CRSD WK'] = pd.to_numeric(df13simple['AUX'])\n",
    "df13simple['Missing Commit Supply Due to CRSD WK'].fillna(0, inplace=True)\n",
    "\n",
    "#ZCM ASN QTY and WK1 Net Supply CV COLUMN\n",
    "df13simple['ZCM ASN QTY and WK1 Net Supply'] = np.where( (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK'] == 'WK1'),\n",
    "                                                           df13simple['ZCM ASN/Commit Net WK1 Supply'],0 )\n",
    "\n",
    "#ZCM Net Commits WK2 CW COLUMN\n",
    "df13simple['ZCM Net Commits WK2'] = np.where( (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK'] == 'WK2'),\n",
    "                                                           df13simple['ZCM Commits WK2'],0 )\n",
    "#ZZCM Net Commits WK3 CX COLUMN\n",
    "df13simple['ZCM Net Commits WK3'] = np.where( (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK'] == 'WK3'),\n",
    "                                                           df13simple['ZCM Commits WK3'],0 )\n",
    "\n",
    "#Net Supply capture by CRSD SKU/DC/WK DC COLUMN  \n",
    "#CV+CW+CX+CY+CZ+DA\n",
    "df13simple['ZCM ASN QTY and WK1 Net Supply'] = pd.to_numeric(df13simple['ZCM ASN QTY and WK1 Net Supply'])\n",
    "df13simple['ZCM Net Commits WK2'] = pd.to_numeric(df13simple['ZCM Net Commits WK2'])\n",
    "df13simple['ZCM Net Commits WK3'] = pd.to_numeric(df13simple['ZCM Net Commits WK3'])\n",
    "\n",
    "df13simple['Net Supply capture by CRSD SKU/DC/WK'] = np.where( df13simple['skuduplicates2']== False,\n",
    "                                                           df13simple['ZCM ASN QTY and WK1 Net Supply'] + df13simple['ZCM Net Commits WK3'] + df13simple['ZCM Net Commits WK2'],0)\n",
    "\n",
    "df13simple['ZCM ASN QTY Supply'] = pd.to_numeric(df13simple['ZCM ASN QTY Supply'])\n",
    "#COLUMN DE\n",
    "df13simple['Missing Commit Supply Due to CRSD WK'] = pd.to_numeric(df13simple['Missing Commit Supply Due to CRSD WK'])\n",
    "\n",
    "###########NEW\n",
    "# COLUMN DD =+IF(Z4<>\"Nothing Missing\",CC4,0)\n",
    "#bring the ZCM ASN QTY Supply to the Missing ASN Supply Due to CRSD WK column\n",
    "#Nothing Missing ACCORDING TO RED COLUMN VALUES\n",
    "\n",
    "df13simple['Missing ASN Supply Due to CRSD WK'] = np.where((df13simple['Missing CRSD Wksimple'] == 'All Missing Blank') & (df13simple['skuduplicates']== False) | (df13simple['Missing CRSD Wksimple'] == 'All Missing WK3') & (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK']=='WK3') \n",
    "                                                    | (df13simple['Missing CRSD Wksimple'] == 'Missing WK1,Missing WK3') & (df13simple['skuduplicates2']== False) |\n",
    "                                                    (df13simple['Missing CRSD Wksimple'] == 'Missing WK1') & (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK']=='WK2') | \n",
    "                                                    (df13simple['Missing CRSD Wksimple'] == 'Missing WK2') & (df13simple['skuduplicates2']== False)& (df13simple['CRSD WK']=='WK3') |\n",
    "                                                    (df13simple['Missing CRSD Wksimple'] == 'Missing WK3') & (df13simple['skuduplicates2']== False) & (df13simple['CRSD WK']==''), \n",
    "                                                    df13simple['ZCM ASN QTY Supply'], 0)\n",
    "\n",
    "df13simple['Missing ASN Supply Due to CRSD WK'] = pd.to_numeric(df13simple['Missing ASN Supply Due to CRSD WK'])\n",
    "\n",
    "\n",
    "\n",
    "#COLUMN DF Last column of green bucket, copied formulas from yesterday's file\n",
    "#DF =IF(OR(AA4=\"WK2\",AA4=\"WK3\",AA4=\"\"),CY4,IF(CY4-CX4<0,CX4,CY4))\n",
    "df13simple['Merlin/Trilogy Missing Supply Total'] = np.where( df13simple['CRSD WK'] != 'WK1',\n",
    "                                            df13simple['Missing Commit Supply Due to CRSD WK'],\n",
    "                                            np.where( df13simple['Missing Commit Supply Due to CRSD WK'] - df13simple['Missing ASN Supply Due to CRSD WK'] < 0,\n",
    "                                            df13simple['Missing ASN Supply Due to CRSD WK'], df13simple['Missing Commit Supply Due to CRSD WK'])) \n",
    "\n",
    "df13simple['Merlin/Trilogy Missing Supply Total'] = np.where( (df13simple['Merlin/Trilogy Missing Supply Total'] > 0) & \n",
    "                                                      ((df13simple['Missing CRSD Wksimple'] == 'Missing WK2') | (df13simple['Missing CRSD Wksimple'] == 'Missing WK3')),\n",
    "                                            df13simple['Missing Commit Supply Due to CRSD WK'],df13simple['Merlin/Trilogy Missing Supply Total'])\n",
    "#print(len(df13simple))\n",
    "\n",
    "#J COLUMN\n",
    "df13simple[\"Material Avail Date > CRSD\"]=np.where((df13simple['SSD'] > df13simple['CRSD']), 'Yes', 'No')\n",
    "df13simple=df13simple.drop(['skuduplicates','skuduplicates2','AUX'], axis=1)\n",
    "\n",
    "## 2ND check POINT MARISOL config printer\n",
    "#CREATE RED COLUMN COMBINED BASE\n",
    "df13simplec=df1[df1['MATL_TYPE'] == 'Simple Config']\n",
    "df13c = pd.crosstab(index=df13simplec[\"Config Printer SKU/DC\"], columns=df13simplec[\"CRSD WK\"], values=df13simplec[\"CRSD WK\"], \n",
    "                    aggfunc='count').fillna(0)\n",
    "df13c = df13c.reset_index()\n",
    "df13c.sort_values(by='Config Printer SKU/DC', ascending=True, inplace=True)\n",
    "\n",
    "df13c['WK1'] = df13c['WK1'].astype(int)\n",
    "df13c['WK2'] = df13c['WK2'].astype(int)\n",
    "df13c['WK3'] = df13c['WK3'].astype(int)\n",
    "#df13.to_excel('RED COLUMN TABLE1.xlsx', index=False)\n",
    "df13c['WK1'] = np.where(df13c['WK1'] > 0, 'Yes', 'No')\n",
    "df13c['WK2'] = np.where(df13c['WK2'] > 0, 'Yes', 'No')\n",
    "df13c['WK3'] = np.where(df13c['WK3'] > 0, 'Yes', 'No')\n",
    "df13c[''] = np.where(df13c[''] > 0, 'Yes', 'No')\n",
    "\n",
    "#df13 = df13.drop([''], axis = 1)\n",
    "df13c['Missing CRSD Wksimple'] = np.where((df13c['WK1'] == 'No') & (df13c['WK2'] == 'No') & (df13c['WK3'] == 'No'), 'All Missing Blank',\n",
    "                          np.where((df13c['WK1'] == 'No') & (df13c['WK2'] == 'Yes') & (df13c['WK3'] == 'No') & (df13c[''] == 'Yes'), 'Missing WK1,Missing WK3',#\n",
    "                          np.where((df13c['WK1'] == 'No') & (df13c['WK2'] == 'No') & (df13c['WK3'] == 'Yes'), 'All Missing WK3',\n",
    "                          np.where((df13c[''] == 'Yes') & (df13c['WK2'] == 'Yes') & (df13c['WK3'] == 'No'), 'Missing WK3',         \n",
    "                          np.where((df13c['WK1'] == 'Yes') & (df13c['WK3'] == 'Yes') & (df13c['WK2'] == 'No'), 'Missing WK2',\n",
    "                          np.where((df13c['WK2'] == 'Yes') & (df13c['WK1'] == 'No'), 'Missing WK1', 'Nothing Missing'))))))\n",
    "\n",
    "#df13c.to_excel('RED COLUMN TABLEconfig.xlsx', index=False)\n",
    "#print(\"Red Column for simple config created\")\n",
    "df13simplec = pd.merge(df13simplec, df13c, on='Config Printer SKU/DC', how='left')\n",
    "\n",
    "#df1 = df1.drop(['WK1','WK2','WK3'], axis = 1)\n",
    "df13simplec['DISTY']=''\n",
    "df13simplec['SLSOFF2']=''\n",
    "df13simplec['PSION']=''\n",
    "df13simplec['COMPLETE']=''\n",
    "df13simplec['IR_NUMBER']=''\n",
    "#print('Reordering columns')\n",
    "df13simplec = df13simplec[[\"DATE\",\"THEATRE\",\"SHIP_LOC\",\"Trilogy Or Merlin\",\"ORD_TYPE\",\"SLSOFF2\",\"SLSOFF2_DE\",\"BOOKED\",\"CRSD\",\n",
    "           \"Material Avail Date > CRSD\",\"SSD\",\"FSSD\",\"DFF_CRSD\",\"SO\",\"LINE\",\"SO_LINE\",\"SO Line Dropped to DC for Delivery\",\n",
    "           \"SKU Short(QTYs On Hand) DBL & CRSD <=+4\",\"SO Short(QTYs On Hand) DBL & CRSD <=+4\",\n",
    "           \"SHIP-SET $'s\",\"ORDER $'s\",\"SHIP_SET\",\"PART\",\"Missing CRSD Wksimple\",\"CRSD WK\",\"TPC SKU/DC/WK Key\",\"SKU DC Key\",\n",
    "           \"Config Printer SKU\",\"Config Printer SKU/DC\",\"Config Printer SKU/DC/WK Key\",\"TPC/ZCM Transmit\",\"MATL_TYPE\",\n",
    "           \"ACTUAL_QTY\",\"REMAIN_QTY\",\"Part ASP $\",\"Blended ASP $\",\n",
    "           \"EXT Part ASP $\",\"UNIT_VAL\",\"TOT_BL\",\"TOP-FILE $'s\",\"SOLDTO_NO\",\"SOLD_TO\",\"SHIP_TO\",\"SHIP_DESC\",\"HOLD\",\n",
    "           \"DISTY\",\"DISTY_1\",\"TOTAL_ORD\",\"FAMILY\",\"PROD_LINE\",\"CORE\",\"COMPLETE\",\"PO\",\"SOURCE\",\"Sourcing Rule\",\"XFR\",\"SOURCE_2\",\n",
    "           \"STATUS\",\"CARRIER\",\"SHIP_MTHD\",\"IR_NUMBER\",\"DAYS_OLD\",\"DAYS_OLD2\",\"BUYER.\",\"PLANNER CODE\",\"BUYER\",\"INVC_QTY\",\n",
    "           \"REMAIN_INV\",\"DISTY2\",\"FSSD_CRSD\",\"HOLD_2\",\"REASON\",\"SHIP_QTY\",\"Org Part INV-Part Simple Config Total QOH\",\n",
    "           \"Config SKU/DC WO Supply Simple Config\",\"Config SKU/DC INV-Simple Config\",\"QOH (Part Simple Config + BASE+ WO Supply)\",\n",
    "           \"QOH\",\"Oracle Intransit(Trilogy IR/ISO)\",\"Oracle Intransit(Trilogy=>Merlin or Merlin=>Trilogy)\",\"ZCM ASN QTY Supply\",\n",
    "           \"ZCM Commits WK1\",\"ZCM Commits WK2\",\"ZCM Commits WK3\",\"ZCM Commits WK4\",\"ZCM Commits WK5\",\"WK1 TPC Receipt QTY\",\n",
    "           \"Valid TPC Supply\",\"TPC Commits WK1\",\"TPC Commits WK2\",\"TPC Commits WK3\",\"TPC Commits WK4\",\"ZCM ASN/Commit Net WK1 Supply\",\n",
    "           \"ZCM ASN QTY and WK1 Net Supply\",\"ZCM Net Commits WK2\",\"ZCM Net Commits WK3\",\"Valid TPC Net Supply WK1\",\n",
    "           \"TPC Net Commits WK2\",\"TPC Net Commits WK3\",\"Missing Commits TPC/ZCM WK2\",\"Net Supply capture by CRSD SKU/DC/WK\",\n",
    "           \"Missing ASN Supply Due to CRSD WK\",\"Missing Commit Supply Due to CRSD WK\",\"Merlin/Trilogy Missing Supply Total\",\n",
    "           \"ORG_BOOK\",\"SKU/DC Total Supply(OH only)\",\"QTY Short to Sales Orders(CUM) Only INV\",\"QTY Short to Sales Orders(NON-CUM) Only INV\",\n",
    "           \"CRSD_WEEK\",\"CRSD_MONTH\",\"CRSD_QTR\",\"SSD_WEEK\",\"SSD_MONTH\",\"SSD_QTR\",\"PSION\",\"COUNTRY\",\"DELINQ\",\"ALIGN\",\"BOOK_WEEK\",\n",
    "           \"BU\",\"SYSTEM\",\"CREATED_BY\",\"DFF_CRSD2\",\"BOOK_CRSD\",\"BOOK_CRSD2\",\"DAYS_OLD3\",\"SUB_FAMILY\",\"CODE_NAME\",\"THEATRE1\",\n",
    "           \"EVM_AIT\",\"HOLD_3\",\"INCO\",\"EVM_AIT2\",\"DELINQ_SSD\",\"DAYS_OLD4\",\"PAY_TERMS\",\"FISC_ZCRD\",\"FISC_SSD\",\"BUSINESS\",\"PRIORITY\",\n",
    "           \"STATE\",\"SUB_LINE\",\"EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM) Only INV)\",\"SKU/DC Total Supply(On Hand+ALL ASN)\",\n",
    "           \"QTY Short to Sales Orders(CUM) (On Hand+ALL ASN\",\"QTY Short to Sales Orders(NON-CUM) On Hand+ALL ASN\",\n",
    "           \"EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM)) On Hand+ALL ASN\",\"SKU/DC Total Supply ALL ASNs+Commits(No Missing Commits)\",\n",
    "           \"QTY Short to Sales Orders(CUM) including Missing Commits+Commits+ALL ASNs\",\"QTY Short to Sales Orders(NON-CUM) w/ Commits+Missing Commits+ALL ASNs\",\n",
    "           \"EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM)) INV w/ Commits+Missing Commits+ALL ASNs\",\"SKU/DC Total Supply(OH+ALL ASNs+all 3wk Commtsh)\",\n",
    "           \"QTY Short to Sales Orders(CUM) OH+ALL ASNs+all 3wk Commts\",\"QTY Short to Sales Orders(NON-CUM) OH+ALL ASNs+all 3wk Commts\",\n",
    "           \"EXT Part ASP $ (QTY Short to Sales Orders(NON-CUM)) OH+ALL ASNs+all 3wk Commts\"]]\n",
    "\n",
    "#print('Done Reordering of columns')\n",
    "df13simplec.sort_values(by=['SKU DC Key','CRSD'],inplace=True)\n",
    "\n",
    "#DE\n",
    "#Auxiliar to keep only first rows, it is false in AUX1, AUX1 answers to the question, is this a duplicate row?\n",
    "df13simplec['skuduplicates']=df13simplec.duplicated(subset=['Config Printer SKU/DC']) #FALSE FOR ALL FIRST APPEARENCE of SKU DC Key,TRUE FOR DUPLICATES\n",
    "#DUPLICATES FOR SKUDC+WK\n",
    "df13simplec['skuduplicates2']=df13simplec.duplicated(subset=['Config Printer SKU/DC/WK Key'])\n",
    "#df1 = pd.merge(df1, df13simple, on='Config Printer SKU/DC', how='left')\n",
    "\n",
    "\n",
    "df13simplec['AUX'] = np.where((df13simplec['Missing CRSD Wksimple'] == 'Missing WK1,Missing WK3') & (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK']== 'WK2'), \n",
    "                            df13simplec['ZCM Commits WK1'],\n",
    "             np.where((df13simplec['Missing CRSD Wksimple'] == 'All Missing WK3') & (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK']== 'WK3'),\n",
    "                      df13simplec['ZCM Commits WK1'] + df13simplec['ZCM Commits WK2'],\n",
    "             np.where((df13simplec['Missing CRSD Wksimple'] == 'All Missing Blank') & (df13simplec['skuduplicates']== False),\n",
    "                      df13simplec['ZCM Commits WK1'] + df13simplec['ZCM Commits WK2'] + df13simplec['ZCM Commits WK3'],\n",
    "             np.where((df13simplec['Missing CRSD Wksimple'] == 'Missing WK1') & (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK']== 'WK2'),\n",
    "                      df13simplec['ZCM Commits WK1'],\n",
    "             np.where((df13simplec['Missing CRSD Wksimple'] == 'Missing WK2') & (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK']== 'WK3'),\n",
    "                      df13simplec['ZCM Commits WK2'],\n",
    "             np.where((df13simplec['Missing CRSD Wksimple'] == 'Missing WK3') & (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK']== ''),\n",
    "                      df13simplec['ZCM Commits WK3'],\n",
    "            np.where((df13simplec['Missing CRSD Wksimple'] == 'Missing WK1,Missing WK3') & (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK']== ''),\n",
    "                      df13simplec['ZCM Commits WK3'],0)))))))\n",
    "#COLUMN ED MUTIPLE VLOOKUPS DEPENDING ON RED COLUMN\n",
    "df13simplec['Missing Commit Supply Due to CRSD WK'] = pd.to_numeric(df13simplec['AUX'])\n",
    "df13simplec['Missing Commit Supply Due to CRSD WK'].fillna(0, inplace=True)\n",
    "\n",
    "#ZCM ASN QTY and WK1 Net Supply CQ COLUMN\n",
    "df13simplec['ZCM ASN QTY and WK1 Net Supply'] = np.where( (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK'] == 'WK1'),\n",
    "                                                           df13simplec['ZCM ASN/Commit Net WK1 Supply'],0 )\n",
    "\n",
    "#ZCM Net Commits WK2 CR COLUMN\n",
    "df13simplec['ZCM Net Commits WK2'] = np.where( (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK'] == 'WK2'),\n",
    "                                                           df13simplec['ZCM Commits WK2'],0 )\n",
    "#ZZCM Net Commits WK3 CS COLUMN\n",
    "df13simplec['ZCM Net Commits WK3'] = np.where( (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK'] == 'WK3'),\n",
    "                                                           df13simplec['ZCM Commits WK3'],0 )\n",
    "\n",
    "#Net Supply capture by CRSD SKU/DC/WK CX COLUMN  \n",
    "#CV+CW+CX+CY+CZ+DA\n",
    "df13simplec['ZCM ASN QTY and WK1 Net Supply'] = pd.to_numeric(df13simplec['ZCM ASN QTY and WK1 Net Supply'])\n",
    "df13simplec['ZCM Net Commits WK2'] = pd.to_numeric(df13simplec['ZCM Net Commits WK2'])\n",
    "df13simplec['ZCM Net Commits WK3'] = pd.to_numeric(df13simplec['ZCM Net Commits WK3'])\n",
    "\n",
    "df13simplec['Net Supply capture by CRSD SKU/DC/WK'] = np.where( df13simplec['skuduplicates2']== False, df13simplec['ZCM ASN QTY and WK1 Net Supply'] + df13simplec['ZCM Net Commits WK3'] + df13simplec['ZCM Net Commits WK2'],0)\n",
    "\n",
    "df13simplec['ZCM ASN QTY Supply'] = pd.to_numeric(df13simplec['ZCM ASN QTY Supply'])\n",
    "#COLUMN DE\n",
    "df13simplec['Missing Commit Supply Due to CRSD WK'] = pd.to_numeric(df13simplec['Missing Commit Supply Due to CRSD WK'])\n",
    "\n",
    "###########NEW\n",
    "# COLUMN DD =+IF(Z4<>\"Nothing Missing\",CC4,0)\n",
    "#bring the ZCM ASN QTY Supply to the Missing ASN Supply Due to CRSD WK column\n",
    "#Nothing Missing ACCORDING TO RED COLUMN VALUES\n",
    "\n",
    "df13simplec['Missing ASN Supply Due to CRSD WK'] = np.where((df13simplec['Missing CRSD Wksimple'] == 'All Missing Blank') & (df13simplec['skuduplicates']== False) | (df13simplec['Missing CRSD Wksimple'] == 'All Missing WK3') & (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK']=='WK3') \n",
    "                                                    | (df13simplec['Missing CRSD Wksimple'] == 'Missing WK1,Missing WK3') & (df13simplec['skuduplicates2']== False) |\n",
    "                                                    (df13simplec['Missing CRSD Wksimple'] == 'Missing WK1') & (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK']=='WK2') | \n",
    "                                                    (df13simplec['Missing CRSD Wksimple'] == 'Missing WK2') & (df13simplec['skuduplicates2']== False)& (df13simplec['CRSD WK']=='WK3') |\n",
    "                                                    (df13simplec['Missing CRSD Wksimple'] == 'Missing WK3') & (df13simplec['skuduplicates2']== False) & (df13simplec['CRSD WK']==''), \n",
    "                                                    df13simplec['ZCM ASN QTY Supply'], 0)\n",
    "\n",
    "df13simplec['Missing ASN Supply Due to CRSD WK'] = pd.to_numeric(df13simplec['Missing ASN Supply Due to CRSD WK'])\n",
    "\n",
    "\n",
    "\n",
    "#COLUMN DF Last column of green bucket, copied formulas from yesterday's file\n",
    "#DF =IF(OR(AA4=\"WK2\",AA4=\"WK3\",AA4=\"\"),CY4,IF(CY4-CX4<0,CX4,CY4))\n",
    "df13simplec['Merlin/Trilogy Missing Supply Total'] = np.where( df13simplec['CRSD WK'] != 'WK1',\n",
    "                                            df13simplec['Missing Commit Supply Due to CRSD WK'],\n",
    "                                            np.where( df13simplec['Missing Commit Supply Due to CRSD WK'] - df13simplec['Missing ASN Supply Due to CRSD WK'] < 0,\n",
    "                                            df13simplec['Missing ASN Supply Due to CRSD WK'], df13simplec['Missing Commit Supply Due to CRSD WK'])) \n",
    "\n",
    "df13simplec['Merlin/Trilogy Missing Supply Total'] = np.where( (df13simplec['Merlin/Trilogy Missing Supply Total'] > 0) & \n",
    "                                                      ((df13simplec['Missing CRSD Wksimple'] == 'Missing WK2') | (df13simplec['Missing CRSD Wksimple'] == 'Missing WK3')),\n",
    "                                            df13simplec['Missing Commit Supply Due to CRSD WK'],df13simplec['Merlin/Trilogy Missing Supply Total'])\n",
    "\n",
    "#J COLUMN\n",
    "df13simplec[\"Material Avail Date > CRSD\"]=np.where((df13simplec['SSD'] > df13simplec['CRSD']), 'Yes', 'No')\n",
    "df13simplec=df13simplec.drop(['skuduplicates','skuduplicates2','AUX'], axis=1)\n",
    "\n",
    "df13simplec.sort_values(by=['Config Printer SKU/DC','CRSD'],inplace=True)\n",
    "print('Writing file')\n",
    "strings = time.strftime(\"%m.%d.%Y\")\n",
    "strings= 'ATS ' + strings \n",
    "b=(\"%s.xlsx\" % strings)\n",
    "\n",
    "with pd.ExcelWriter(b) as writer:  \n",
    "    #dfASN.to_excel(writer, sheet_name='CRM',index= False)\n",
    "    df13simple.to_excel(writer, sheet_name='COMBINED',index= False,startrow=2)\n",
    "    df13simplec.to_excel(writer, sheet_name='COMBINED BASE',index= False,startrow=2)\n",
    "    \n",
    "c=(time.time() - start_time)/60\n",
    "print(\"--- %s minutes to create AIT BL---\" % c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=fnmatch.filter(os.listdir('.'), 'All Commit_ZCM*')\n",
    "a=a[0]\n",
    "#print(a)\n",
    "#print(b)\n",
    "pathtc = \"C:\\\\Users\\\\MJ7255\\\\Desktop\\\\Backlog\\\\Backlog Report SAM\\\\Backlog Report\\\\Untitled Folder\\\\\" + a\n",
    "pathf = \"C:\\\\Users\\\\MJ7255\\\\Desktop\\\\Backlog\\\\Backlog Report SAM\\\\Backlog Report\\\\Untitled Folder\\\\\" + b\n",
    "wbtc = xw.Book(pathtc)\n",
    "wbf = xw.Book(pathf)\n",
    "wstc = wbtc.sheets(1)\n",
    "\n",
    "wstc.api.Copy(After=wbf.sheets(2).api)\n",
    "wbf.save()\n",
    "wbtc.app.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1\n",
      "   a  b  c\n",
      "0  1  1  1\n",
      "1  2  2  2\n",
      "2  3  3  3\n",
      "3  4  4  4\n",
      "\n",
      "df2\n",
      "   a  c\n",
      "0  1  1\n",
      "1  2  2\n",
      "2  3  3\n",
      "3  4  4\n",
      "\n",
      "df\n",
      "   a    b  c\n",
      "0  1  1.0  1\n",
      "1  2  2.0  2\n",
      "2  3  3.0  3\n",
      "3  4  4.0  4\n",
      "0  1  NaN  1\n",
      "1  2  NaN  2\n",
      "2  3  NaN  3\n",
      "3  4  NaN  4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys, os\n",
    "import glob\n",
    "import time\n",
    "import fnmatch\n",
    "import os\n",
    "import xlwings as xw\n",
    "from datetime import date\n",
    "df1 = pd.DataFrame({'a':[1,2,3,4],'b':[1,2,3,4],'c':[1,2,3,4]})\n",
    "df2 = pd.DataFrame({'a':[1,2,3,4],'c':[1,2,3,4]})\n",
    "\n",
    "df = pd.concat([df1,df2])\n",
    "\n",
    "print('df1')\n",
    "print(df1)\n",
    "print('\\ndf2')\n",
    "print(df2)\n",
    "print('\\ndf')\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
